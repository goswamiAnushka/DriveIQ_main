{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after loading: 47846\n",
      "Rows after dropping NaNs: 42601\n",
      "Rows after processing trips: 42601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\AppData\\Local\\Temp\\ipykernel_29340\\3939980232.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('TripID', group_keys=False).apply(process_trip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after outlier removal: 41494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\AppData\\Local\\Temp\\ipykernel_29340\\3939980232.py:120: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['Heading'] = df.groupby('TripID', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2, degrees\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the driving data\n",
    "df = pd.read_csv('../data/driving_data.csv')\n",
    "print(\"Rows after loading:\", len(df))\n",
    "\n",
    "\n",
    "# Ensure that Latitude, Longitude, and TimeStamp are in the correct format\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "df['TimeStamp'] = pd.to_datetime(df['TimeStamp'].str.replace('EDT', ''), errors='coerce')  # Remove timezone for simplicity\n",
    "\n",
    "# Drop rows with missing values in important columns\n",
    "df = df.dropna(subset=['Latitude', 'Longitude', 'TimeStamp'])\n",
    "print(\"Rows after dropping NaNs:\", len(df))\n",
    "\n",
    "\n",
    "# Sort the data by TripID and TimeStamp to ensure trips are processed in the correct order\n",
    "df = df.sort_values(by=['TripID', 'TimeStamp']).reset_index(drop=True)\n",
    "print(\"Rows after processing trips:\", len(df))\n",
    "\n",
    "# Function for calculating distance using the Haversine formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    d_lat = radians(lat2 - lat1)\n",
    "    d_lon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "    \n",
    "    a = sin(d_lat / 2)**2 + cos(lat1) * cos(lat2) * sin(d_lon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c * 1000  # Distance in meters\n",
    "\n",
    "# Processing each trip separately\n",
    "def process_trip(trip_df):\n",
    "    trip_df['Time_Diff'] = trip_df['TimeStamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    # Shift latitude and longitude for distance calculation\n",
    "    trip_df['Lat_Shifted'] = trip_df['Latitude'].shift(1)\n",
    "    trip_df['Lon_Shifted'] = trip_df['Longitude'].shift(1)\n",
    "\n",
    "    # Calculate distance between consecutive points using the Haversine formula\n",
    "    trip_df['Distance(m)'] = trip_df.apply(lambda row: haversine(row['Lat_Shifted'], row['Lon_Shifted'], row['Latitude'], row['Longitude']), axis=1)\n",
    "\n",
    "    # Calculate speed (m/s) using distance and time difference\n",
    "    trip_df['Speed(m/s)'] = trip_df['Distance(m)'] / trip_df['Time_Diff'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "    # Calculate acceleration (m/s²)\n",
    "    trip_df['Acceleration(m/s^2)'] = trip_df['Speed(m/s)'].diff() / trip_df['Time_Diff'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "    # Calculate jerk (m/s³)\n",
    "    trip_df['Jerk(m/s^3)'] = trip_df['Acceleration(m/s^2)'].diff() / trip_df['Time_Diff'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "    # Calculate braking intensity (absolute value of negative acceleration)\n",
    "    trip_df['Braking_Intensity'] = trip_df['Acceleration(m/s^2)'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "\n",
    "    # Fill missing values after calculations\n",
    "    trip_df.fillna(0, inplace=True)\n",
    "\n",
    "    return trip_df\n",
    "\n",
    "# Apply the processing for each TripID separately while keeping the trip order\n",
    "df = df.groupby('TripID', group_keys=False).apply(process_trip)\n",
    "\n",
    "# Ensure the DataFrame is sorted by TripID and TimeStamp after processing\n",
    "df = df.sort_values(by=['TripID', 'TimeStamp']).reset_index(drop=True)\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "z_scores = np.abs(stats.zscore(df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity']].fillna(0)))\n",
    "df = df[(z_scores < 3).all(axis=1)]  # Keep data within 3 standard deviations\n",
    "print(\"Rows after outlier removal:\", len(df))\n",
    "\n",
    "# Load sensitive locations (school, hospital, etc.)\n",
    "sensitive_locations = pd.read_csv('../data/sensitive_location.csv')\n",
    "\n",
    "# Function to calculate SASV (Sensitive Area Speed Violation)\n",
    "def haversine_vectorized(lat1, lon1, lat2_series, lon2_series):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    d_lat = np.radians(lat2_series - lat1)\n",
    "    d_lon = np.radians(lon2_series - lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2_series = np.radians(lat2_series)\n",
    "    a = np.sin(d_lat / 2)**2 + np.cos(lat1) * np.cos(lat2_series) * np.sin(d_lon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c * 1000  # Distance in meters\n",
    "\n",
    "def calculate_sasv(lat, lon, speed, sensitive_locations, sasv_threshold=8.33, radius_threshold=300):\n",
    "    sensitive_distances = haversine_vectorized(lat, lon, sensitive_locations['Latitude'], sensitive_locations['Longitude'])\n",
    "    if np.any(sensitive_distances < radius_threshold):  # Within 300 meters of sensitive areas\n",
    "        if speed > sasv_threshold:  # Speed > 30 km/h in sensitive area\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Apply SASV calculation\n",
    "df['SASV'] = df.apply(lambda row: calculate_sasv(row['Latitude'], row['Longitude'], row['Speed(m/s)'], sensitive_locations), axis=1)\n",
    "\n",
    "# Calculate rule violation score for exceeding general speed limit\n",
    "def calculate_speed_violation(row, speed_limit=13.89):  # Default 50 km/h\n",
    "    if row['Speed(m/s)'] > speed_limit:\n",
    "        return 1  # Speed violation\n",
    "    return 0\n",
    "\n",
    "df['Speed_Violation'] = df.apply(calculate_speed_violation, axis=1)\n",
    "\n",
    "# ---------- Calculate Heading and Heading Change ---------- #\n",
    "def calculate_heading(lat1, lon1, lat2, lon2):\n",
    "    d_lon = lon2 - lon1\n",
    "    x = sin(radians(d_lon)) * cos(radians(lat2))\n",
    "    y = cos(radians(lat1)) * sin(radians(lat2)) - sin(radians(lat1)) * cos(radians(lat2)) * cos(radians(d_lon))\n",
    "    initial_bearing = atan2(x, y)\n",
    "    initial_bearing = degrees(initial_bearing)\n",
    "    return (initial_bearing + 360) % 360  # Normalize to 0-360\n",
    "\n",
    "df['Heading'] = df.groupby('TripID', group_keys=False).apply(\n",
    "    lambda group: group.apply(\n",
    "        lambda row: calculate_heading(row['Lat_Shifted'], row['Lon_Shifted'], row['Latitude'], row['Longitude']), axis=1\n",
    "    )\n",
    ").reset_index(level=0, drop=True)\n",
    "df['Heading_Change(degrees)'] = df['Heading'].diff().fillna(0)\n",
    "\n",
    "# ---------- Driving Score Calculation ---------- #\n",
    "df['Driving_Score'] = 100\n",
    "\n",
    "# Save unnormalized values before normalization\n",
    "df['Speed_Unnormalized(m/s)'] = df['Speed(m/s)']\n",
    "df['Acceleration_Unnormalized(m/s^2)'] = df['Acceleration(m/s^2)']\n",
    "df['Jerk_Unnormalized(m/s^3)'] = df['Jerk(m/s^3)']\n",
    "df['Braking_Intensity_Unnormalized'] = df['Braking_Intensity']\n",
    "df['Heading_Change_Unnormalized'] = df['Heading_Change(degrees)']\n",
    "\n",
    "# Normalize the key features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity', 'Heading_Change(degrees)']] = scaler.fit_transform(\n",
    "    df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity', 'Heading_Change(degrees)']])\n",
    "\n",
    "# Apply penalties based on normalized features\n",
    "df['Driving_Score'] -= df['Speed(m/s)'] * 25  # Speed penalty\n",
    "df['Driving_Score'] -= df['Acceleration(m/s^2)'] * 20  # Acceleration penalty\n",
    "df['Driving_Score'] -= df['Jerk(m/s^3)'] * 10  # Jerk penalty\n",
    "df['Driving_Score'] -= df['Braking_Intensity'] * 5  # Braking intensity penalty\n",
    "df['Driving_Score'] -= df['Heading_Change(degrees)'] * 5  # Heading change penalty\n",
    "\n",
    "# Penalty for violations\n",
    "df['Driving_Score'] -= df['SASV'] * 10  # Penalty for violating sensitive areas\n",
    "df['Driving_Score'] -= df['Speed_Violation'] * 15  # Penalty for general speed limit violations\n",
    "\n",
    "# Categorize driving behavior\n",
    "def categorize_driving(score):\n",
    "    if score >= 80:\n",
    "        return 'Safe'\n",
    "    elif 60 <= score < 80:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Risky'\n",
    "\n",
    "df['Driving_Category'] = df['Driving_Score'].apply(categorize_driving)\n",
    "\n",
    "# Save the processed data to CSV, keeping only relevant columns\n",
    "processed_columns = ['TripID', 'TimeStamp', 'Speed_Unnormalized(m/s)', 'Speed(m/s)',\n",
    "                     'Acceleration_Unnormalized(m/s^2)', 'Acceleration(m/s^2)',\n",
    "                     'Jerk_Unnormalized(m/s^3)', 'Jerk(m/s^3)', \n",
    "                     'Braking_Intensity_Unnormalized', 'Braking_Intensity', \n",
    "                     'SASV', 'Speed_Violation', 'Heading', \n",
    "                     'Heading_Change_Unnormalized', 'Heading_Change(degrees)', \n",
    "                     'Driving_Score', 'Driving_Category']\n",
    "\n",
    "df[processed_columns].to_csv('../data/processed_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
